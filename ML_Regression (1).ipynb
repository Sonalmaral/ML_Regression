{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Den4lYuBGV2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tWhat is Simple Linear Regression?\n",
        ">> Simple Linear Regression (SLR) models the relationship between a single independent variable XXX and a dependent variable YYY, assuming a linear relationship:\n",
        "                Y=mX+cY = mX + cY=mX+c\n",
        "2.\tWhat are the key assumptions of Simple Linear Regression?\n",
        ">> o\tLinearity: YYY is linearly related to XXX.\n",
        "o\tIndependence: Observations are independent.\n",
        "o\tHomoscedasticity: Constant variance of errors.\n",
        "o\tNormality: Errors are normally distributed.\n",
        "3.\tWhat does the coefficient mmm represent in the equation Y=mX+cY = mX + cY=mX+c?\n",
        ">> m is the slope, showing the change in YYY for a one-unit increase in XXX.\n",
        "4.\tWhat does the intercept ccc represent in the equation Y=mX+cY = mX + cY=mX+c?\n",
        ">> c is the intercept, the predicted value of YYY when X=0X = 0X=0.\n",
        "5.\tHow do we calculate the slope mmm in Simple Linear Regression?\n",
        ">> m=∑(Xi−Xˉ)2∑(Xi−Xˉ)(Yi−Yˉ)\n",
        "6.\tWhat is the purpose of the least squares method in Simple Linear Regression?\n",
        ">> It minimizes the sum of squared residuals (differences between observed and predicted YYY) to find the best-fitting line.\n",
        "7.\tHow is the coefficient of determination (R2R^2R2) interpreted in Simple Linear Regression?\n",
        ">> It represents the proportion of variance in Y explained by X.\n",
        "8.\tMain difference between Simple and Multiple Linear Regression?\n",
        ">> o\tSLR: 1 independent variable\n",
        "o\tMLR: 2 or more independent variables\n",
        "9.\tKey assumptions of Multiple Linear Regression:\n",
        ">> o\tLinearity\n",
        "o\tIndependence\n",
        "o\tHomoscedasticity\n",
        "o\tNo multicollinearity\n",
        "o\tNormality of residuals\n",
        "11.\tWhat is heteroscedasticity, and how does it affect results?\n",
        ">> It's when the variance of residuals isn't constant. It can lead to inefficient estimates and biased standard errors, affecting hypothesis testing.\n",
        "12.\tImproving a model with high multicollinearity:\n",
        ">> a.\tRemove or combine correlated predictors\n",
        "b.\tUse Principal Component Analysis (PCA)\n",
        "c.\tApply Ridge or Lasso regression\n",
        "13.\tTransforming categorical variables:\n",
        ">> a.\tOne-Hot Encoding\n",
        "b.\tLabel Encoding\n",
        "c.\tDummy Variables\n",
        "14.\tRole of interaction terms:\n",
        ">> They capture the combined effect of two variables, showing how the relationship between one predictor and the response changes with another predictor.\n",
        "15.\tIntercept interpretation differences:\n",
        "16.\tSignificance of the slope:\n",
        ">> ndicates how much the dependent variable changes per unit increase in a predictor; helps in making predictions.\n",
        "17.\tContext provided by the intercept:\n",
        ">> Acts as a baseline value; essential in understanding predictions when input values are zero (though may not always be realistic).\n",
        "18.\tLimitations of using R2R^2R2 alone:\n",
        ">> a.\tDoesn’t indicate model complexity\n",
        "b.\tCan’t detect overfitting\n",
        "c.\tDoesn’t reflect predictive power on new data\n",
        "Use Adjusted R2R^2R2 or cross-validation scores.\n",
        "19.\tInterpreting a large standard error for a coefficient:\n",
        ">> Suggests that the estimate is unstable or not statistically significant; may indicate multicollinearity or small sample size.\n",
        "20.\tIdentifying heteroscedasticity in residual plots:\n",
        ">> Look for funnel shapes or patterns instead of randomness.\n",
        "It's crucial to address because it violates regression assumptions and affects inference accuracy.\n",
        "21.\tHigh R2R^2R2 but low Adjusted R2R^2R2:\n",
        ">> Indicates that added predictors do not improve the model and may be unnecessary (overfitting risk).\n",
        "22.\tWhy scale variables in MLR:\n",
        ">> a.\tEnsures fair comparison among predictors\n",
        "b.\tEssential for regularization methods\n",
        "c.\tHelps in improving numerical stability\n",
        "23.\tWhat is polynomial regression?\n",
        ">> A form of regression where the relationship between the independent variable and the dependent variable is modeled as an nth-degree polynomial:\n",
        "24.\tDifference from linear regression:\n",
        ">> Polynomial regression captures non-linear relationships, whereas linear regression models straight-line relationships.\n",
        "25.\tWhen is polynomial regression used?\n",
        ">> o\tWhen data shows a curved pattern\n",
        "o\tTo model non-linear trends while keeping the model linear in terms of parameters\n",
        "26.\tGeneral equation:\n",
        "\n",
        "27.\tCan it be applied to multiple variables?\n",
        ">> Yes, it's called multivariate polynomial regression, and it includes cross-terms and powers of multiple predictors.\n",
        "28.\tLimitations:\n",
        ">> o\tProne to overfitting\n",
        "o\tSensitive to outliers\n",
        "o\tCan produce erratic predictions outside training range (poor extrapolation)\n",
        "29.\tModel fit evaluation techniques:\n",
        ">> o\tAdjusted R2R^2R2\n",
        "o\tCross-validation\n",
        "o\tAIC/BIC\n",
        "o\tResidual analysis\n",
        "30.\tWhy is visualization important?\n",
        ">> Helps detect overfitting/underfitting and reveals the nature of the curve fitting the data.\n",
        "31.\tPolynomial regression in Python:\n",
        ">> python\n",
        "CopyEdit\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 6, 14, 28, 45])\n",
        "\n",
        "# Transform to polynomial features\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.title(\"Polynomial Regression (degree=2)\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "HRXP9XrCGYBF"
      }
    }
  ]
}